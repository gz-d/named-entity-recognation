{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64026492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3babdb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when quoting=3, '\"' will be interpreted as an ordinary char, not a quote character\n",
    "train = pd.read_csv('hw4/data/train', sep=' ', quoting=3, names=['index', 'word', 'ner'])\n",
    "dev = pd.read_csv('hw4/data/dev', sep=' ', quoting=3, names=['index', 'word', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c5be795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training data into individual training samples\n",
    "word_train = []\n",
    "ner_train = []\n",
    "\n",
    "word = []\n",
    "ner = []\n",
    "\n",
    "for i in range(len(train)-1):\n",
    "    if train.loc[i]['index'] < train.loc[i+1]['index']:\n",
    "        word.append(train.loc[i]['word'])\n",
    "        ner.append(train.loc[i]['ner'])\n",
    "    else:\n",
    "        word.append(train.loc[i]['word'])\n",
    "        ner.append(train.loc[i]['ner'])\n",
    "        word_train.append(word)\n",
    "        ner_train.append(ner)\n",
    "        word = []\n",
    "        ner = []\n",
    "\n",
    "# split dev data into individual samples\n",
    "word_dev = []\n",
    "ner_dev = []\n",
    "\n",
    "word = []\n",
    "ner = []\n",
    "\n",
    "for i in range(len(dev)-1):\n",
    "    if dev.loc[i]['index'] < dev.loc[i+1]['index']:\n",
    "        word.append(dev.loc[i]['word'])\n",
    "        ner.append(dev.loc[i]['ner'])\n",
    "    else:\n",
    "        word.append(dev.loc[i]['word'])\n",
    "        ner.append(dev.loc[i]['ner'])\n",
    "        word_dev.append(word)\n",
    "        ner_dev.append(ner)\n",
    "        word = []\n",
    "        ner = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af8474ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], ['Peter', 'Blackburn'], ['BRUSSELS', '1996-08-22'], ['The', 'European', 'Commission', 'said', 'on', 'Thursday', 'it', 'disagreed', 'with', 'German', 'advice', 'to', 'consumers', 'to', 'shun', 'British', 'lamb', 'until', 'scientists', 'determine', 'whether', 'mad', 'cow', 'disease', 'can', 'be', 'transmitted', 'to', 'sheep', '.'], ['Germany', \"'s\", 'representative', 'to', 'the', 'European', 'Union', \"'s\", 'veterinary', 'committee', 'Werner', 'Zwingmann', 'said', 'on', 'Wednesday', 'consumers', 'should', 'buy', 'sheepmeat', 'from', 'countries', 'other', 'than', 'Britain', 'until', 'the', 'scientific', 'advice', 'was', 'clearer', '.']]\n",
      "[['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O'], ['B-PER', 'I-PER'], ['B-LOC', 'O'], ['O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['B-LOC', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n"
     ]
    }
   ],
   "source": [
    "print(word_train[:5])\n",
    "print(ner_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c2ad70",
   "metadata": {},
   "source": [
    "## Task1: Simple Bidirectional LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "653f9090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vocabulary using the training data\n",
    "vocab = train['word'].unique().tolist()\n",
    "\n",
    "# Add a token '<unk>' to the vocabulary in case that there is unknown words\n",
    "# in the dev and testing data\n",
    "vocab.append('<unk>')\n",
    "# Add a token '<pad>' to the vocabulary to pad shorter sentences in a batch\n",
    "vocab.append('<pad>')\n",
    "\n",
    "# Create a dictionary with integer as key and named entity as value\n",
    "ner_ls = train['ner'].unique().tolist()\n",
    "# Add to ner_ls a fake ne 'PAD' corresponding to the '<pad>' in the vocabulary\n",
    "ner_ls.append('PAD')\n",
    "ner_dict, ner_dict2 = {}, {}\n",
    "for i, ner in enumerate(ner_ls):\n",
    "    ner_dict[ner] = i\n",
    "    ner_dict2[i] = ner\n",
    "\n",
    "# Convert the ner_train and ner_dev to label_train and label_dev\n",
    "label_train = [[ner_dict[ner] for ner in ner_train[i]] for i in range(len(ner_train))]\n",
    "label_dev = [[ner_dict[ner] for ner in ner_dev[i]] for i in range(len(ner_dev))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "115c1321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-ORG',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'B-PER',\n",
       " 'I-PER',\n",
       " 'B-LOC',\n",
       " 'I-ORG',\n",
       " 'I-MISC',\n",
       " 'I-LOC',\n",
       " 'PAD']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8cd9644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-ORG': 0,\n",
       " 'O': 1,\n",
       " 'B-MISC': 2,\n",
       " 'B-PER': 3,\n",
       " 'I-PER': 4,\n",
       " 'B-LOC': 5,\n",
       " 'I-ORG': 6,\n",
       " 'I-MISC': 7,\n",
       " 'I-LOC': 8,\n",
       " 'PAD': 9}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dbac40d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'B-ORG',\n",
       " 1: 'O',\n",
       " 2: 'B-MISC',\n",
       " 3: 'B-PER',\n",
       " 4: 'I-PER',\n",
       " 5: 'B-LOC',\n",
       " 6: 'I-ORG',\n",
       " 7: 'I-MISC',\n",
       " 8: 'I-LOC',\n",
       " 9: 'PAD'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_dict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e5294fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import functools as fc\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56c157c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a custom dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels, vocab):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # For each word in the vocab, we will represent it using a one-hot vector.\n",
    "        # For instance, for the first word 'EU' in the vocabulary, its one-hot vector\n",
    "        # will be [1, 0, 0, ..., 0], and the size of the one-hot vector is the size of\n",
    "        # the vocabualry\n",
    "        \n",
    "        sentence = self.features[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        vectors = []\n",
    "        # keep track of word index\n",
    "        word_ids = []\n",
    "        \n",
    "        for _, word in enumerate(sentence):\n",
    "            vector = [0 for i in range(len(self.vocab))]\n",
    "            # handle (possibly unknown words in the dev or testing data)\n",
    "            if word in vocab:\n",
    "                vector[vocab.index(word)] = 1\n",
    "                word_id = vocab.index(word)\n",
    "            else:\n",
    "                vector[vocab.index('<unk>')] = 1\n",
    "                word_id = vocab.index('<unk>')\n",
    "            vectors.append(vector)\n",
    "            word_ids.append(word_id)\n",
    "        vectors = torch.Tensor(vectors)\n",
    "        \n",
    "        return vectors, label, word_ids     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f440414c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a collate_fn to pad sequences in a batch to the same length\n",
    "# https://discuss.pytorch.org/t/how-to-create-batches-of-a-list-of-varying-dimension-tensors/50773/14\n",
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "       data: is a list of tuples with (vectors, label, word_ids) returned\n",
    "             the CustomDataset, where vectors is a tensor with shape of\n",
    "             seq_len * word_embeddings, label a list of named entities,\n",
    "             and word_ids the word indices in the vocabulary.  \n",
    "    \"\"\"\n",
    "    labels = [data[i][1] for i in range(len(data))]\n",
    "    word_ids = [data[i][2] for i in range(len(data))]\n",
    "    # the length of an example corresponds to the first dimension of its tensor\n",
    "    lengths = [data[i][0].size(0) for i in range(len(data))]\n",
    "    max_len = max(lengths)\n",
    "    # the number of features is the second dimension of the tensor:\n",
    "    num_features = data[0][0].size(1)\n",
    "    # initialize a zero tensor of shape batch_size * max_len * n_ftrs\n",
    "    features = torch.zeros((len(data), max_len, num_features))\n",
    "    \n",
    "    # pad the tensor of each sequence with zeros, the label of each sequence with the\n",
    "    # label of 'PAD', and the index of each sequence with the index of '<pad>' in the\n",
    "    # vocabulary.\n",
    "    for i in range(len(data)):\n",
    "        j = data[i][0].size(0)\n",
    "        features[i] = torch.cat([data[i][0], torch.zeros((max_len-j, num_features))])\n",
    "        labels[i] = labels[i] + [ner_dict['PAD'] for _ in range(max_len-j)]\n",
    "        word_ids[i] = word_ids[i] + [vocab.index('<pad>') for _ in range(max_len-j)]\n",
    "    labels = torch.Tensor(labels)\n",
    "    word_ids = torch.Tensor(word_ids)\n",
    "    lengths = torch.Tensor(lengths)\n",
    "    \n",
    "    return features.float(), labels.long(), lengths.long(), word_ids.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "162846f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataloader for training data\n",
    "train_dataset = CustomDataset(word_train, label_train, vocab)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True,\n",
    "                              collate_fn=collate_fn)\n",
    "\n",
    "# create a dataloader for dev data\n",
    "dev_dataset = CustomDataset(word_dev, label_dev, vocab)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=64, shuffle=True,\n",
    "                            collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4fd76601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the architecture of the simple bidirectional LSTM model\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_dim=len(vocab), embed_dim=100, lstm_dim=256, \n",
    "                 dropout=0.33, linear_dim=128, output_dim=len(ner_ls)):\n",
    "        super().__init__()\n",
    "        # dimension of inputs\n",
    "        self.input_dim = input_dim\n",
    "        # dimension of embeddings\n",
    "        self.embed_dim = embed_dim\n",
    "        # linear layer from input one hot vectors to word embeddings\n",
    "        self.fc1 = nn.Linear(self.input_dim, self.embed_dim)\n",
    "        # dimension of the lstm layer\n",
    "        self.lstm_dim = lstm_dim\n",
    "        # lstm layer\n",
    "        self.lstm = nn.LSTM(self.embed_dim, self.lstm_dim, batch_first=True,\n",
    "                            dropout=0.33, bidirectional=True)\n",
    "        # dimension of the linear layer following lstm\n",
    "        self.linear_dim = linear_dim\n",
    "        # linear layer following lstm\n",
    "        self.fc2 = nn.Linear(self.lstm_dim * 2, self.linear_dim)\n",
    "        # elu layer following linear layer\n",
    "        self.elu = nn.ELU()\n",
    "        # dimension of outputs\n",
    "        self.output_dim = output_dim\n",
    "        # linear layer following elu\n",
    "        self.fc3 = nn.Linear(self.linear_dim, self.output_dim)\n",
    "        \n",
    "    def forward(self, batched_data, lengths):\n",
    "        \n",
    "        # batched data is the tensor loaded from dataloader\n",
    "        # shape: batch_size * sequence length * feature number\n",
    "        batch_size = batched_data.shape[0]\n",
    "        \n",
    "        # convert one hot vectors of words in a sequence to word embeddings\n",
    "        batched_data = self.fc1(batched_data[:, :,])\n",
    "        \n",
    "        # pack padded sequences to their original lengths\n",
    "        # lengths must be on cpu even if it is a tensor. Therefore, in case\n",
    "        # that lengths is on gpu, we need to transfer it (with batched_data)\n",
    "        # to cpu, and transfer packed_data back to gpu\n",
    "        packed_data = pack_padded_sequence(batched_data, lengths,\n",
    "                                           batch_first=True,\n",
    "                                           enforce_sorted=False)\n",
    "        h0 = torch.randn(2, batch_size, self.lstm_dim)\n",
    "        c0 = torch.randn(2, batch_size, self.lstm_dim)\n",
    "        '''if lengths.is_cuda:\n",
    "            batched_data, lengths = batched_data.cpu(), lengths.cpu()\n",
    "            packed_data = pack_padded_sequence(batched_data, lengths,\n",
    "                                               batch_first=True,\n",
    "                                               enforce_sorted=False)\n",
    "            packed_data = packed_data.to(device)\n",
    "        else:\n",
    "            packed_data = pack_padded_sequence(batched_data, lengths,\n",
    "                                               batch_first=True,\n",
    "                                               enforce_sorted=False)\n",
    "        \n",
    "        h0 = torch.randn(2, batch_size, self.lstm_dim).to(device)\n",
    "        c0 = torch.randn(2, batch_size, self.lstm_dim).to(device)'''\n",
    "        \n",
    "        # the output contains the output features (h_t) and has a shape of \n",
    "        # batch_size * seq_len * (2(for bidirectional)*num_lstm_layers)\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "        output, (hn, cn) = self.lstm(packed_data, (h0, c0))\n",
    "        # the output is a PackedSequence object, need to convert it back to\n",
    "        # a Variable object using pad_packed_sequence\n",
    "        output, _ = pad_packed_sequence(output, batch_first=True)\n",
    "            \n",
    "        # pass hidden state through linear layer following lstm\n",
    "        linear_out = self.fc2(output)\n",
    "            \n",
    "        # pass output from linear layer through elu\n",
    "        elu_out = self.elu(linear_out)\n",
    "            \n",
    "        # pass output from elu through another linear layer\n",
    "        # the shape of results: batch_size * seq_len * num_of_ners\n",
    "        results = self.fc3(elu_out)\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e109862f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=23304, out_features=100, bias=True)\n",
      "  (lstm): LSTM(100, 256, batch_first=True, dropout=0.33, bidirectional=True)\n",
      "  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (elu): ELU(alpha=1.0)\n",
      "  (fc3): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guanz\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:58: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.33 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "# initialize a BiLSTM model\n",
    "model = BiLSTM()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5dd2fcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify cross entropy loss as loss function\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=ner_dict['PAD'])\n",
    "\n",
    "# specify optimizer (stochastic gradient descent) and initial learning rate = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# adjust learning rate using learning rate scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, min_lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f22665f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# train model on gpu\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "adb1968a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.470560 \tValidation Loss: 0.478189\n",
      "Validation loss decreased (0.500000 --> 0.478189).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.460265 \tValidation Loss: 0.738701\n",
      "Epoch: 3 \tTraining Loss: 0.453596 \tValidation Loss: 0.439980\n",
      "Validation loss decreased (0.478189 --> 0.439980).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.440506 \tValidation Loss: 0.451953\n",
      "Epoch: 5 \tTraining Loss: 0.430057 \tValidation Loss: 0.505304\n",
      "Epoch: 6 \tTraining Loss: 0.422528 \tValidation Loss: 0.419714\n",
      "Validation loss decreased (0.439980 --> 0.419714).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.410976 \tValidation Loss: 0.529148\n",
      "Epoch: 8 \tTraining Loss: 0.403119 \tValidation Loss: 0.412609\n",
      "Validation loss decreased (0.419714 --> 0.412609).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 0.386645 \tValidation Loss: 0.399781\n",
      "Validation loss decreased (0.412609 --> 0.399781).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.381126 \tValidation Loss: 0.419596\n",
      "Epoch: 11 \tTraining Loss: 0.370905 \tValidation Loss: 0.737982\n",
      "Epoch: 12 \tTraining Loss: 0.366086 \tValidation Loss: 0.387876\n",
      "Validation loss decreased (0.399781 --> 0.387876).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 0.345963 \tValidation Loss: 0.380979\n",
      "Validation loss decreased (0.387876 --> 0.380979).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 0.351338 \tValidation Loss: 0.366666\n",
      "Validation loss decreased (0.380979 --> 0.366666).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 0.329224 \tValidation Loss: 0.411616\n",
      "Epoch: 16 \tTraining Loss: 0.330992 \tValidation Loss: 0.405975\n",
      "Epoch: 17 \tTraining Loss: 0.324971 \tValidation Loss: 0.364801\n",
      "Validation loss decreased (0.366666 --> 0.364801).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 0.319392 \tValidation Loss: 0.376037\n",
      "Epoch: 19 \tTraining Loss: 0.300562 \tValidation Loss: 0.346620\n",
      "Validation loss decreased (0.364801 --> 0.346620).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 0.299004 \tValidation Loss: 0.483050\n",
      "Epoch: 21 \tTraining Loss: 0.289729 \tValidation Loss: 0.704262\n",
      "Epoch: 22 \tTraining Loss: 0.290008 \tValidation Loss: 0.332995\n",
      "Validation loss decreased (0.346620 --> 0.332995).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 0.279848 \tValidation Loss: 0.349100\n",
      "Epoch: 24 \tTraining Loss: 0.271714 \tValidation Loss: 0.323409\n",
      "Validation loss decreased (0.332995 --> 0.323409).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 0.267722 \tValidation Loss: 0.509703\n",
      "Epoch: 26 \tTraining Loss: 0.268587 \tValidation Loss: 0.331954\n",
      "Epoch: 27 \tTraining Loss: 0.272678 \tValidation Loss: 0.317251\n",
      "Validation loss decreased (0.323409 --> 0.317251).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 0.265251 \tValidation Loss: 0.320618\n",
      "Epoch: 29 \tTraining Loss: 0.242423 \tValidation Loss: 0.352541\n",
      "Epoch: 30 \tTraining Loss: 0.250391 \tValidation Loss: 0.323698\n",
      "Epoch: 31 \tTraining Loss: 0.238238 \tValidation Loss: 0.309184\n",
      "Validation loss decreased (0.317251 --> 0.309184).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 0.250584 \tValidation Loss: 0.358802\n",
      "Epoch: 33 \tTraining Loss: 0.231982 \tValidation Loss: 0.339789\n",
      "Epoch: 34 \tTraining Loss: 0.223858 \tValidation Loss: 2.960524\n",
      "Epoch: 35 \tTraining Loss: 0.363487 \tValidation Loss: 0.322244\n",
      "Epoch: 36 \tTraining Loss: 0.217221 \tValidation Loss: 0.296863\n",
      "Validation loss decreased (0.309184 --> 0.296863).  Saving model ...\n",
      "Epoch: 37 \tTraining Loss: 0.222917 \tValidation Loss: 0.331089\n",
      "Epoch: 38 \tTraining Loss: 0.224038 \tValidation Loss: 0.292567\n",
      "Validation loss decreased (0.296863 --> 0.292567).  Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 0.222103 \tValidation Loss: 0.310704\n",
      "Epoch: 40 \tTraining Loss: 0.204282 \tValidation Loss: 0.288417\n",
      "Validation loss decreased (0.292567 --> 0.288417).  Saving model ...\n",
      "Epoch: 41 \tTraining Loss: 0.201675 \tValidation Loss: 0.320474\n",
      "Epoch: 42 \tTraining Loss: 0.219252 \tValidation Loss: 0.299517\n",
      "Epoch: 43 \tTraining Loss: 0.204797 \tValidation Loss: 0.367425\n",
      "Epoch: 44 \tTraining Loss: 0.192744 \tValidation Loss: 0.316835\n",
      "Epoch: 45 \tTraining Loss: 0.185650 \tValidation Loss: 0.329424\n",
      "Epoch: 46 \tTraining Loss: 0.190116 \tValidation Loss: 0.293760\n",
      "Epoch: 47 \tTraining Loss: 0.167535 \tValidation Loss: 0.290973\n",
      "Epoch: 48 \tTraining Loss: 0.165559 \tValidation Loss: 0.294980\n",
      "Epoch: 49 \tTraining Loss: 0.162919 \tValidation Loss: 0.307818\n",
      "Epoch: 50 \tTraining Loss: 0.160832 \tValidation Loss: 0.479071\n",
      "Epoch: 51 \tTraining Loss: 0.161317 \tValidation Loss: 0.276055\n",
      "Validation loss decreased (0.288417 --> 0.276055).  Saving model ...\n",
      "Epoch: 52 \tTraining Loss: 0.157271 \tValidation Loss: 0.291484\n",
      "Epoch: 53 \tTraining Loss: 0.155219 \tValidation Loss: 0.272511\n",
      "Validation loss decreased (0.276055 --> 0.272511).  Saving model ...\n",
      "Epoch: 54 \tTraining Loss: 0.156082 \tValidation Loss: 0.318074\n",
      "Epoch: 55 \tTraining Loss: 0.151717 \tValidation Loss: 0.401402\n",
      "Epoch: 56 \tTraining Loss: 0.154763 \tValidation Loss: 0.273893\n",
      "Epoch: 57 \tTraining Loss: 0.148217 \tValidation Loss: 0.272845\n",
      "Epoch: 58 \tTraining Loss: 0.147256 \tValidation Loss: 0.286681\n",
      "Epoch: 59 \tTraining Loss: 0.144522 \tValidation Loss: 0.269905\n",
      "Validation loss decreased (0.272511 --> 0.269905).  Saving model ...\n",
      "Epoch: 60 \tTraining Loss: 0.143019 \tValidation Loss: 0.320297\n",
      "Epoch: 61 \tTraining Loss: 0.146271 \tValidation Loss: 0.316462\n",
      "Epoch: 62 \tTraining Loss: 0.138572 \tValidation Loss: 0.520363\n",
      "Epoch: 63 \tTraining Loss: 0.137686 \tValidation Loss: 0.304424\n",
      "Epoch: 64 \tTraining Loss: 0.139894 \tValidation Loss: 0.306546\n",
      "Epoch: 65 \tTraining Loss: 0.139712 \tValidation Loss: 0.290670\n",
      "Epoch: 66 \tTraining Loss: 0.128069 \tValidation Loss: 0.271003\n",
      "Epoch: 67 \tTraining Loss: 0.127295 \tValidation Loss: 0.266668\n",
      "Validation loss decreased (0.269905 --> 0.266668).  Saving model ...\n",
      "Epoch: 68 \tTraining Loss: 0.125943 \tValidation Loss: 0.280060\n",
      "Epoch: 69 \tTraining Loss: 0.125168 \tValidation Loss: 0.265617\n",
      "Validation loss decreased (0.266668 --> 0.265617).  Saving model ...\n",
      "Epoch: 70 \tTraining Loss: 0.124580 \tValidation Loss: 0.280957\n",
      "Epoch: 71 \tTraining Loss: 0.122584 \tValidation Loss: 0.291667\n",
      "Epoch: 72 \tTraining Loss: 0.122478 \tValidation Loss: 0.275774\n",
      "Epoch: 73 \tTraining Loss: 0.121050 \tValidation Loss: 0.305101\n",
      "Epoch: 74 \tTraining Loss: 0.120050 \tValidation Loss: 0.270958\n",
      "Epoch: 75 \tTraining Loss: 0.119224 \tValidation Loss: 0.298276\n",
      "Epoch: 76 \tTraining Loss: 0.117216 \tValidation Loss: 0.268260\n",
      "Epoch: 77 \tTraining Loss: 0.116719 \tValidation Loss: 0.277838\n",
      "Epoch: 78 \tTraining Loss: 0.116777 \tValidation Loss: 0.271914\n",
      "Epoch: 79 \tTraining Loss: 0.116745 \tValidation Loss: 0.267243\n",
      "Epoch: 80 \tTraining Loss: 0.115488 \tValidation Loss: 0.272238\n",
      "Epoch: 81 \tTraining Loss: 0.115403 \tValidation Loss: 0.284788\n",
      "Epoch: 82 \tTraining Loss: 0.114021 \tValidation Loss: 0.281813\n",
      "Epoch: 83 \tTraining Loss: 0.114883 \tValidation Loss: 0.270348\n",
      "Epoch: 84 \tTraining Loss: 0.114120 \tValidation Loss: 0.283703\n",
      "Epoch: 85 \tTraining Loss: 0.114847 \tValidation Loss: 0.289611\n",
      "Epoch: 86 \tTraining Loss: 0.113527 \tValidation Loss: 0.271930\n",
      "Epoch: 87 \tTraining Loss: 0.113386 \tValidation Loss: 0.279154\n",
      "Epoch: 88 \tTraining Loss: 0.112473 \tValidation Loss: 0.279840\n",
      "Epoch: 89 \tTraining Loss: 0.113114 \tValidation Loss: 0.273727\n",
      "Epoch: 90 \tTraining Loss: 0.113165 \tValidation Loss: 0.272467\n",
      "Epoch: 91 \tTraining Loss: 0.112761 \tValidation Loss: 0.272724\n",
      "Epoch: 92 \tTraining Loss: 0.113299 \tValidation Loss: 0.274642\n",
      "Epoch: 93 \tTraining Loss: 0.112848 \tValidation Loss: 0.281443\n",
      "Epoch: 94 \tTraining Loss: 0.112545 \tValidation Loss: 0.274604\n",
      "Epoch: 95 \tTraining Loss: 0.111967 \tValidation Loss: 0.279352\n",
      "Epoch: 96 \tTraining Loss: 0.112444 \tValidation Loss: 0.276172\n",
      "Epoch: 97 \tTraining Loss: 0.111481 \tValidation Loss: 0.275544\n",
      "Epoch: 98 \tTraining Loss: 0.112389 \tValidation Loss: 0.276502\n",
      "Epoch: 99 \tTraining Loss: 0.112147 \tValidation Loss: 0.274323\n",
      "Epoch: 100 \tTraining Loss: 0.112604 \tValidation Loss: 0.276396\n"
     ]
    }
   ],
   "source": [
    "# transfer model to GPU\n",
    "# model.to(device)\n",
    "\n",
    "# train the network\n",
    "max_epochs = 100\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ## train the model\n",
    "    model.train()\n",
    "    \n",
    "    for features, labels, lengths, _ in train_dataloader:\n",
    "        \n",
    "        # transfer to GPU\n",
    "        #features, labels, lengths = features.to(device), labels.to(device), lengths.to(device)\n",
    "        \n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass: compute predictions by passing inputs to the model\n",
    "        output = model(features, lengths)\n",
    "        \n",
    "        # the shape of output: batch_size * seq_len * num_of_ners\n",
    "        # need: batch_size * num_of_ners * seq_len to fit in the loss function\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "        output = torch.transpose(output, 1, 2)\n",
    "        \n",
    "        # calculate the loss\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # update running training loss\n",
    "        train_loss += loss.item() * features.size(0)\n",
    "        \n",
    "    ## validate the model\n",
    "    model.eval()\n",
    "    \n",
    "    for features, labels, lengths, _ in dev_dataloader:\n",
    "        # transfer to GPU\n",
    "        #features, labels, lengths = features.to(device), labels.to(device), lengths.to(device)\n",
    "        \n",
    "        # forward pass\n",
    "        output = model(features, lengths)\n",
    "        \n",
    "        # the shape of output: batch_size * seq_len * num_of_ners\n",
    "        # need: batch_size * num_of_ners * seq_len to fit in the loss function\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "        output = torch.transpose(output, 1, 2)\n",
    "        \n",
    "        # calculate the loss\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        # update running validation loss\n",
    "        valid_loss += loss.item() * features.size(0)\n",
    "    \n",
    "    # scheduler should be called after validation\n",
    "    scheduler.step(valid_loss)\n",
    "    \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_dataloader.dataset)\n",
    "    valid_loss = valid_loss/len(dev_dataloader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        valid_loss\n",
    "        ))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'model.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ed143e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataloader):\n",
    "    preds = torch.Tensor()\n",
    "    labels = torch.Tensor()\n",
    "    word_ids = torch.Tensor()\n",
    "    lengths = torch.Tensor()\n",
    "    for features, label, length, word_id in dataloader:\n",
    "        # the shape of output: batch_size * seq_len * num_of_ners\n",
    "        output = model(features, length)\n",
    "        # torch.max returns values (0) and indices (1) of the max, here we\n",
    "        # only need indices (1)\n",
    "        _, pred = torch.max(output, 2)\n",
    "        # Paddings should be excluded from the computation of the prediction\n",
    "        # accuracy\n",
    "        pred = pack_padded_sequence(pred, length, batch_first=True,\n",
    "                                    enforce_sorted=False).data\n",
    "        label = pack_padded_sequence(label, length, batch_first=True,\n",
    "                                    enforce_sorted=False).data\n",
    "        word_id = pack_padded_sequence(word_id, length, batch_first=True,\n",
    "                                    enforce_sorted=False).data\n",
    "        # concatenate batches\n",
    "        preds = torch.cat((preds, pred))\n",
    "        labels = torch.cat((labels, label))\n",
    "        word_ids = torch.cat((word_ids, word_id))\n",
    "        lengths = torch.cat((lengths, length))\n",
    "    preds = preds.reshape(-1).tolist()\n",
    "    labels = labels.reshape(-1).tolist()\n",
    "    word_ids = word_ids.reshape(-1).tolist()\n",
    "    lengths = lengths.reshape(-1).tolist()\n",
    "    return preds, labels, lengths, word_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ab373f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('model.pt'))\n",
    "preds, labels, lengths, word_ids = predict(model, dev_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eced36c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a results list holding the required information (idx, word, gold, pred)\n",
    "# results is a list of sublists, with sublist being a list of (idx, word, gold, pred)\n",
    "# for each word in a sentence\n",
    "results = []\n",
    "pos = 0\n",
    "for _, length in enumerate(lengths):\n",
    "    # sentence-level result\n",
    "    result = []\n",
    "    idx = 0\n",
    "    for i in range(pos, pos+int(length)):\n",
    "        # word-level result\n",
    "        result_word = [idx, vocab[int(word_ids[i])], ner_dict2[int(labels[i])], ner_dict2[int(preds[i])]]\n",
    "        result.append(result_word)\n",
    "        idx += 1\n",
    "    results.append(result)\n",
    "    pos += int(length)\n",
    "\n",
    "# convert results (a list of sublists) to a single list\n",
    "results = fc.reduce(lambda a, b: a + b, results)\n",
    "# convert results to a dataframe\n",
    "results = pd.DataFrame(data=results, columns=['idx', 'word', 'gold', 'pred'])\n",
    "# write results to a file with the required format:\n",
    "# idx word gold pred\n",
    "results.to_csv('recap/dev_results', sep=' ', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "945ea42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert results (a list of sublists) to a single list\n",
    "results = fc.reduce(lambda a, b: a + b, results)\n",
    "# convert results to a dataframe\n",
    "results = pd.DataFrame(data=results, columns=['idx', 'word', 'gold', 'pred'])\n",
    "# write results to a file with the required format:\n",
    "# idx word gold pred\n",
    "results.to_csv('recap/dev_results', sep=' ', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e297525f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6877308453562512"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(labels, preds, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a4bc0b22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.933765075425602"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(labels, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db68db9",
   "metadata": {},
   "source": [
    "## Task 2: Using GloVe word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "567f345c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import the pretrained glove model\n",
    "\n",
    "# each line in the glove file is in the format:\n",
    "# word dim0 dim1 dim2 ...\n",
    "# for example:\n",
    "# after 0.38315 -0.3561 -0.1283 -0.19527 0.047629...\n",
    "glove = {}\n",
    "with open('hw4/glove.6B.100d/glove.6B.100d.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], 'float32')\n",
    "        glove[word] = vector\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79219cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a vocabulary from glove\n",
    "vocab_glove = list(glove.keys())\n",
    "# for words that are included in the training data but not in vocab_glove, that is, \n",
    "# 1) words whose case-insensitive counterparts are included in the vocab_glove\n",
    "# 2) words whose case-insensitive counterparts are not included in the vocab_glove,\n",
    "# we will add them to vocab_glove\n",
    "for word in train['word'].unique().tolist():\n",
    "    if word not in vocab_glove:\n",
    "        vocab_glove.append(word)\n",
    "\n",
    "# Add a token '<unk>' to the vocabulary in case that there is unknown words\n",
    "# in the dev and testing data\n",
    "vocab_glove.append('<unk>')\n",
    "# Add a token '<pad>' to the vocabulary to pad shorter sentences in a batch\n",
    "vocab_glove.append('<pad>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8e8bdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# because the pretrain glove model is case-insensitive, while for ner task,\n",
    "# case sensitivity is critical. Therefore, we can train the embedding for\n",
    "# those words (e.g. China) based on the embedding of their case-insensitive\n",
    "# counterparts (e.g. china) in the glove. That is, for words in glove, we use\n",
    "# the embedding from glove, while for words having their counterparts in\n",
    "# glove, we will initialize their embedding with the embedding of their\n",
    "# counterparts, and train it during the training of the entire lstm model.\n",
    "# in order to do this, we are going to modify the custom dataset class used\n",
    "# in the previous task:\n",
    "\n",
    "# create a custom dataset compatible with glove\n",
    "class CustomDataset2(Dataset):\n",
    "    def __init__(self, features, labels, vocab, wvm):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.wvm = wvm # wvm is the word embedding model (e.g. glove), in a dictionary\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # For words that the glove contains, we will use the pretrained word embedding.\n",
    "        # For words whose case-insensitive counterparts are in the glove, we will\n",
    "        # initialize their word embedding with the word embedding of their counterparts.\n",
    "        # For words whose case-insensitive counterparts are not in the glove, we will\n",
    "        # initialize their word embedding randomly.\n",
    "        # For instance, word 'after' is in the glove, so we will use the pretrained\n",
    "        # word embedding glove['after']; word 'China' is not in the glove, but 'china'\n",
    "        # is, so we will initialize the word bedding for 'China' with glove['china'],\n",
    "        # and update it during the training stage; word 'nihao' is not in the glove,\n",
    "        # we will initialize its word embedding randomly.\n",
    "        \n",
    "        sentence = self.features[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        vectors = []\n",
    "        # keep track of word index\n",
    "        word_ids = []\n",
    "        \n",
    "        for _, word in enumerate(sentence):\n",
    "            # handle (possibly unknown words in the dev or testing data)\n",
    "            if word in self.vocab:\n",
    "                if word in self.wvm.keys():\n",
    "                    vector = self.wvm[word]\n",
    "                    word_id = self.vocab.index(word)\n",
    "                elif word.lower() in self.wvm.keys():\n",
    "                    vector = self.wvm[word.lower()]\n",
    "                    word_id = self.vocab.index(word)\n",
    "                else:\n",
    "                    vector = np.random.rand(100)\n",
    "                    word_id = self.vocab.index(word)\n",
    "            else:\n",
    "                vector = np.random.rand(100)\n",
    "                word_id = self.vocab.index('<unk>')\n",
    "                \n",
    "            vectors.append(vector)\n",
    "            word_ids.append(word_id)   \n",
    "        vectors = torch.Tensor(vectors)\n",
    "        \n",
    "        return vectors, label, word_ids  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fc01b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a collate_fn to pad sequences in a batch to the same length\n",
    "# The collate_fn is almost identical to the one we defined for the simple bidirectional lstm model,\n",
    "# the only difference is that the vocabulary here is vocab_glove, rather than vocab.\n",
    "# https://discuss.pytorch.org/t/how-to-create-batches-of-a-list-of-varying-dimension-tensors/50773/14\n",
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "       data: is a list of tuples with (vectors, label, word_ids) returned\n",
    "             the CustomDataset, where vectors is a tensor with shape of\n",
    "             seq_len * word_embeddings, label a list of named entities,\n",
    "             and word_ids the word indices in the vocabulary.  \n",
    "    \"\"\"\n",
    "    labels = [data[i][1] for i in range(len(data))]\n",
    "    word_ids = [data[i][2] for i in range(len(data))]\n",
    "    # the length of an example corresponds to the first dimension of its tensor\n",
    "    lengths = [data[i][0].size(0) for i in range(len(data))]\n",
    "    max_len = max(lengths)\n",
    "    # the number of features is the second dimension of the tensor:\n",
    "    num_features = data[0][0].size(1)\n",
    "    # initialize a zero tensor of shape batch_size * max_len * n_ftrs\n",
    "    features = torch.zeros((len(data), max_len, num_features))\n",
    "    \n",
    "    # pad the tensor of each sequence with zeros, the label of each sequence with the\n",
    "    # label of 'PAD', and the index of each sequence with the index of '<pad>' in the\n",
    "    # vocabulary.\n",
    "    for i in range(len(data)):\n",
    "        j = data[i][0].size(0)\n",
    "        features[i] = torch.cat([data[i][0], torch.zeros((max_len-j, num_features))])\n",
    "        labels[i] = labels[i] + [ner_dict['PAD'] for _ in range(max_len-j)]\n",
    "        word_ids[i] = word_ids[i] + [vocab_glove.index('<pad>') for _ in range(max_len-j)]\n",
    "    labels = torch.Tensor(labels)\n",
    "    word_ids = torch.Tensor(word_ids)\n",
    "    lengths = torch.Tensor(lengths)\n",
    "    \n",
    "    return features.float(), labels.long(), lengths.long(), word_ids.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cce28487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataloader for training data\n",
    "train_dataset = CustomDataset2(word_train, label_train, vocab_glove, glove)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True,\n",
    "                              collate_fn=collate_fn)\n",
    "\n",
    "# create a dataloader for dev data\n",
    "dev_dataset = CustomDataset2(word_dev, label_dev, vocab_glove, glove)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=64, shuffle=True,\n",
    "                            collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56715baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTM(\n",
      "  (fc1): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (lstm): LSTM(100, 256, batch_first=True, dropout=0.33, bidirectional=True)\n",
      "  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (elu): ELU(alpha=1.0)\n",
      "  (fc3): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guanz\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:58: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.33 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "# initialize a glove-based BiLSTM model and its architecture is the same\n",
    "# as the simple BiLSTM model.\n",
    "# because the dimension of pretrained word embedding in glove is also 100,\n",
    "# we need to assign it to input_dim\n",
    "model_glove = BiLSTM(input_dim=100)\n",
    "print(model_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64e5082f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify cross entropy loss as loss function\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=ner_dict['PAD'])\n",
    "\n",
    "# specify optimizer (stochastic gradient descent) and initial learning rate = 0.1\n",
    "optimizer = torch.optim.SGD(model_glove.parameters(), lr=0.1)\n",
    "\n",
    "# adjust learning rate using learning rate scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, min_lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a00e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.317559 \tValidation Loss: 0.387928\n",
      "Validation loss decreased (0.413378 --> 0.387928).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.298991 \tValidation Loss: 0.402841\n",
      "Epoch: 3 \tTraining Loss: 0.283400 \tValidation Loss: 0.367822\n",
      "Validation loss decreased (0.387928 --> 0.367822).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.270355 \tValidation Loss: 0.350736\n",
      "Validation loss decreased (0.367822 --> 0.350736).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.259955 \tValidation Loss: 0.338315\n",
      "Validation loss decreased (0.350736 --> 0.338315).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.249380 \tValidation Loss: 0.329682\n",
      "Validation loss decreased (0.338315 --> 0.329682).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.241916 \tValidation Loss: 0.318010\n",
      "Validation loss decreased (0.329682 --> 0.318010).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 0.233156 \tValidation Loss: 0.320361\n",
      "Epoch: 9 \tTraining Loss: 0.225202 \tValidation Loss: 0.309085\n",
      "Validation loss decreased (0.318010 --> 0.309085).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.219409 \tValidation Loss: 0.332140\n",
      "Epoch: 11 \tTraining Loss: 0.213807 \tValidation Loss: 0.290706\n",
      "Validation loss decreased (0.309085 --> 0.290706).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 0.206754 \tValidation Loss: 0.285298\n",
      "Validation loss decreased (0.290706 --> 0.285298).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 0.202256 \tValidation Loss: 0.777733\n",
      "Epoch: 14 \tTraining Loss: 0.216271 \tValidation Loss: 0.300734\n",
      "Epoch: 15 \tTraining Loss: 0.193139 \tValidation Loss: 0.278622\n",
      "Validation loss decreased (0.285298 --> 0.278622).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 0.188487 \tValidation Loss: 0.273147\n",
      "Validation loss decreased (0.278622 --> 0.273147).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 0.183862 \tValidation Loss: 0.267991\n",
      "Validation loss decreased (0.273147 --> 0.267991).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 0.180190 \tValidation Loss: 0.275150\n",
      "Epoch: 19 \tTraining Loss: 0.176514 \tValidation Loss: 0.268374\n",
      "Epoch: 20 \tTraining Loss: 0.173892 \tValidation Loss: 0.271283\n",
      "Epoch: 21 \tTraining Loss: 0.171220 \tValidation Loss: 0.265720\n",
      "Validation loss decreased (0.267991 --> 0.265720).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 0.169083 \tValidation Loss: 0.257793\n",
      "Validation loss decreased (0.265720 --> 0.257793).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 0.164432 \tValidation Loss: 0.263624\n",
      "Epoch: 24 \tTraining Loss: 0.162420 \tValidation Loss: 0.274780\n",
      "Epoch: 25 \tTraining Loss: 0.158691 \tValidation Loss: 0.274948\n",
      "Epoch: 26 \tTraining Loss: 0.156933 \tValidation Loss: 0.280186\n",
      "Epoch: 27 \tTraining Loss: 0.155835 \tValidation Loss: 0.245092\n",
      "Validation loss decreased (0.257793 --> 0.245092).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 0.152841 \tValidation Loss: 0.264307\n",
      "Epoch: 29 \tTraining Loss: 0.150619 \tValidation Loss: 0.306733\n",
      "Epoch: 30 \tTraining Loss: 0.148057 \tValidation Loss: 0.241804\n",
      "Validation loss decreased (0.245092 --> 0.241804).  Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 0.146148 \tValidation Loss: 0.228965\n",
      "Validation loss decreased (0.241804 --> 0.228965).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 0.144651 \tValidation Loss: 0.219820\n",
      "Validation loss decreased (0.228965 --> 0.219820).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 0.140785 \tValidation Loss: 0.225192\n",
      "Epoch: 34 \tTraining Loss: 0.139719 \tValidation Loss: 0.234955\n",
      "Epoch: 35 \tTraining Loss: 0.136923 \tValidation Loss: 0.233091\n",
      "Epoch: 36 \tTraining Loss: 0.135781 \tValidation Loss: 0.227525\n",
      "Epoch: 37 \tTraining Loss: 0.133827 \tValidation Loss: 0.221148\n",
      "Epoch: 38 \tTraining Loss: 0.131436 \tValidation Loss: 0.215226\n",
      "Validation loss decreased (0.219820 --> 0.215226).  Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 0.130223 \tValidation Loss: 0.225825\n",
      "Epoch: 40 \tTraining Loss: 0.127918 \tValidation Loss: 0.222422\n",
      "Epoch: 41 \tTraining Loss: 0.127768 \tValidation Loss: 0.209925\n",
      "Validation loss decreased (0.215226 --> 0.209925).  Saving model ...\n",
      "Epoch: 42 \tTraining Loss: 0.124771 \tValidation Loss: 0.206717\n",
      "Validation loss decreased (0.209925 --> 0.206717).  Saving model ...\n",
      "Epoch: 43 \tTraining Loss: 0.123461 \tValidation Loss: 0.221726\n",
      "Epoch: 44 \tTraining Loss: 0.121528 \tValidation Loss: 0.218498\n",
      "Epoch: 45 \tTraining Loss: 0.120889 \tValidation Loss: 0.234665\n",
      "Epoch: 46 \tTraining Loss: 0.118947 \tValidation Loss: 0.205167\n",
      "Validation loss decreased (0.206717 --> 0.205167).  Saving model ...\n",
      "Epoch: 47 \tTraining Loss: 0.117074 \tValidation Loss: 0.200970\n",
      "Validation loss decreased (0.205167 --> 0.200970).  Saving model ...\n",
      "Epoch: 48 \tTraining Loss: 0.115216 \tValidation Loss: 0.214181\n",
      "Epoch: 49 \tTraining Loss: 0.114515 \tValidation Loss: 0.205893\n",
      "Epoch: 50 \tTraining Loss: 0.112943 \tValidation Loss: 0.208443\n",
      "Epoch: 51 \tTraining Loss: 0.111209 \tValidation Loss: 0.202955\n"
     ]
    }
   ],
   "source": [
    "# transfer model to GPU\n",
    "# model_glove.to(device)\n",
    "\n",
    "# train the network\n",
    "max_epochs = 100\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ## train the model\n",
    "    model_glove.train()\n",
    "    \n",
    "    for features, labels, lengths, _ in train_dataloader:\n",
    "        \n",
    "        # transfer to GPU\n",
    "        #features, labels, lengths = features.to(device), labels.to(device), lengths.to(device)\n",
    "        \n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass: compute predictions by passing inputs to the model\n",
    "        output = model_glove(features, lengths)\n",
    "        \n",
    "        # the shape of output: batch_size * seq_len * num_of_ners\n",
    "        # need: batch_size * num_of_ners * seq_len to fit in the loss function\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "        output = torch.transpose(output, 1, 2)\n",
    "        \n",
    "        # calculate the loss\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # update running training loss\n",
    "        train_loss += loss.item() * features.size(0)\n",
    "        \n",
    "    ## validate the model\n",
    "    model_glove.eval()\n",
    "    \n",
    "    for features, labels, lengths, _ in dev_dataloader:\n",
    "        # transfer to GPU\n",
    "        #features, labels, lengths = features.to(device), labels.to(device), lengths.to(device)\n",
    "        \n",
    "        # forward pass\n",
    "        output = model_glove(features, lengths)\n",
    "        \n",
    "        # the shape of output: batch_size * seq_len * num_of_ners\n",
    "        # need: batch_size * num_of_ners * seq_len to fit in the loss function\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "        output = torch.transpose(output, 1, 2)\n",
    "        \n",
    "        # calculate the loss\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        # update running validation loss\n",
    "        valid_loss += loss.item() * features.size(0)\n",
    "    \n",
    "    # scheduler should be called after validation\n",
    "    scheduler.step(valid_loss)\n",
    "    \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_dataloader.dataset)\n",
    "    valid_loss = valid_loss/len(dev_dataloader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        valid_loss\n",
    "        ))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model_glove.state_dict(), 'model_glove.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2686eded",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_glove.load_state_dict(torch.load('model_glove.pt'))\n",
    "preds, labels, lengths, word_ids = predict(model_glove, dev_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a54d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a results list holding the required information (idx, word, gold, pred)\n",
    "# results is a list of sublists, with sublist being a list of (idx, word, gold, pred)\n",
    "# for each word in a sentence\n",
    "results = []\n",
    "pos = 0\n",
    "for _, length in enumerate(lengths):\n",
    "    # sentence-level result\n",
    "    result = []\n",
    "    idx = 0\n",
    "    for i in range(pos, pos+int(length)):\n",
    "        # word-level result\n",
    "        result_word = [idx, vocab_glove[int(word_ids[i])], ner_dict2[int(labels[i])], ner_dict2[int(preds[i])]]\n",
    "        result.append(result_word)\n",
    "        idx += 1\n",
    "    results.append(result)\n",
    "    pos += int(length)\n",
    "\n",
    "# convert results (a list of sublists) to a single list\n",
    "results = fc.reduce(lambda a, b: a + b, results)\n",
    "# convert results to a dataframe\n",
    "results = pd.DataFrame(data=results, columns=['idx', 'word', 'gold', 'pred'])\n",
    "# write results to a file with the required format:\n",
    "# idx word gold pred\n",
    "results.to_csv('dev_results2', sep=' ', header=False, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
